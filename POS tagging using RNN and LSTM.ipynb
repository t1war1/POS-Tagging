{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using nltk treebank corpus\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructuring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "sentence_tags =[]\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence=[]\n",
    "    tags=[]\n",
    "    for (s,w) in tagged_sentence:\n",
    "        sentence.append(s)\n",
    "        tags.append(w)\n",
    "    sentences.append(np.array(sentence))\n",
    "    sentence_tags.append(np.array(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Rudolph', 'Agnew', ',', '55', 'years', 'old', 'and', 'former',\n",
       "       'chairman', 'of', 'Consolidated', 'Gold', 'Fields', 'PLC', ',',\n",
       "       'was', 'named', '*-1', 'a', 'nonexecutive', 'director', 'of',\n",
       "       'this', 'British', 'industrial', 'conglomerate', '.'], dtype='<U12')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', 'CC', 'JJ', 'NN', 'IN',\n",
       "       'NNP', 'NNP', 'NNP', 'NNP', ',', 'VBD', 'VBN', '-NONE-', 'DT',\n",
       "       'JJ', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', '.'], dtype='<U6')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tags[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_sentences, test_sentences, train_tags, test_tags)=train_test_split(sentences, sentence_tags, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3131"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing word to index and tag to index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#',\n",
       " '$',\n",
       " \"''\",\n",
       " ',',\n",
       " '-LRB-',\n",
       " '-NONE-',\n",
       " '-RRB-',\n",
       " '.',\n",
       " ':',\n",
       " 'CC',\n",
       " 'CD',\n",
       " 'DT',\n",
       " 'EX',\n",
       " 'FW',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'JJR',\n",
       " 'JJS',\n",
       " 'LS',\n",
       " 'MD',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NNPS',\n",
       " 'NNS',\n",
       " 'PDT',\n",
       " 'POS',\n",
       " 'PRP',\n",
       " 'PRP$',\n",
       " 'RB',\n",
       " 'RBR',\n",
       " 'RBS',\n",
       " 'RP',\n",
       " 'SYM',\n",
       " 'TO',\n",
       " 'UH',\n",
       " 'VB',\n",
       " 'VBD',\n",
       " 'VBG',\n",
       " 'VBN',\n",
       " 'VBP',\n",
       " 'VBZ',\n",
       " 'WDT',\n",
       " 'WP',\n",
       " 'WP$',\n",
       " 'WRB',\n",
       " '``'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for out-of-vocabulary words \n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing sentences and tags to integers\n",
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    "\n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    train_sentences_X.append(s_int)\n",
    "    \n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    test_sentences_X.append(s_int)\n",
    "    \n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    "    \n",
    "for s in test_tags:\n",
    "    test_tags_y.append([tag2index[t] for t in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert sequence of tags to sequence of one hot encoded tags\n",
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#padding all sequences\n",
    "MAX_LENGTH=0\n",
    "for s in sentences:\n",
    "    MAX_LENGTH=max(MAX_LENGTH,len(s))\n",
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gauravtiwari\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model1.add(Embedding(len(word2index), 128))\n",
    "model1.add(SimpleRNN(256, return_sequences=True))\n",
    "model1.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model1.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    " \n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy',ignore_class_accuracy(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 271, 128)          1306496   \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 271, 256)          98560     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 271, 47)           12079     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 271, 47)           0         \n",
      "=================================================================\n",
      "Total params: 1,417,135\n",
      "Trainable params: 1,417,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2504 samples, validate on 627 samples\n",
      "Epoch 1/40\n",
      "2504/2504 [==============================] - 9s 4ms/step - loss: 0.0422 - acc: 0.9930 - ignore_accuracy: 0.9272 - val_loss: 0.0702 - val_acc: 0.9851 - val_ignore_accuracy: 0.8533\n",
      "Epoch 2/40\n",
      "2504/2504 [==============================] - 9s 4ms/step - loss: 0.0395 - acc: 0.9935 - ignore_accuracy: 0.9326 - val_loss: 0.0678 - val_acc: 0.9855 - val_ignore_accuracy: 0.8578\n",
      "Epoch 3/40\n",
      "2504/2504 [==============================] - 9s 4ms/step - loss: 0.0370 - acc: 0.9939 - ignore_accuracy: 0.9371 - val_loss: 0.0656 - val_acc: 0.9859 - val_ignore_accuracy: 0.8618\n",
      "Epoch 4/40\n",
      "2504/2504 [==============================] - 11s 4ms/step - loss: 0.0347 - acc: 0.9943 - ignore_accuracy: 0.9412 - val_loss: 0.0635 - val_acc: 0.9863 - val_ignore_accuracy: 0.8652\n",
      "Epoch 5/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0325 - acc: 0.9946 - ignore_accuracy: 0.9444 - val_loss: 0.0618 - val_acc: 0.9865 - val_ignore_accuracy: 0.8683\n",
      "Epoch 6/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0305 - acc: 0.9950 - ignore_accuracy: 0.9480 - val_loss: 0.0599 - val_acc: 0.9868 - val_ignore_accuracy: 0.8711\n",
      "Epoch 7/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0287 - acc: 0.9952 - ignore_accuracy: 0.9509 - val_loss: 0.0583 - val_acc: 0.9871 - val_ignore_accuracy: 0.8734\n",
      "Epoch 8/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0270 - acc: 0.9955 - ignore_accuracy: 0.9538 - val_loss: 0.0569 - val_acc: 0.9873 - val_ignore_accuracy: 0.8761\n",
      "Epoch 9/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0254 - acc: 0.9958 - ignore_accuracy: 0.9566 - val_loss: 0.0555 - val_acc: 0.9875 - val_ignore_accuracy: 0.8781\n",
      "Epoch 10/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0240 - acc: 0.9960 - ignore_accuracy: 0.9589 - val_loss: 0.0543 - val_acc: 0.9877 - val_ignore_accuracy: 0.8803\n",
      "Epoch 11/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0227 - acc: 0.9962 - ignore_accuracy: 0.9607 - val_loss: 0.0531 - val_acc: 0.9879 - val_ignore_accuracy: 0.8824\n",
      "Epoch 12/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0215 - acc: 0.9964 - ignore_accuracy: 0.9629 - val_loss: 0.0520 - val_acc: 0.9881 - val_ignore_accuracy: 0.8842\n",
      "Epoch 13/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0203 - acc: 0.9965 - ignore_accuracy: 0.9642 - val_loss: 0.0512 - val_acc: 0.9882 - val_ignore_accuracy: 0.8854\n",
      "Epoch 14/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0193 - acc: 0.9966 - ignore_accuracy: 0.9657 - val_loss: 0.0503 - val_acc: 0.9883 - val_ignore_accuracy: 0.8868\n",
      "Epoch 15/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0184 - acc: 0.9968 - ignore_accuracy: 0.9672 - val_loss: 0.0495 - val_acc: 0.9884 - val_ignore_accuracy: 0.8878\n",
      "Epoch 16/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0175 - acc: 0.9969 - ignore_accuracy: 0.9687 - val_loss: 0.0488 - val_acc: 0.9885 - val_ignore_accuracy: 0.8886\n",
      "Epoch 17/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0167 - acc: 0.9970 - ignore_accuracy: 0.9699 - val_loss: 0.0481 - val_acc: 0.9886 - val_ignore_accuracy: 0.8894\n",
      "Epoch 18/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0159 - acc: 0.9972 - ignore_accuracy: 0.9710 - val_loss: 0.0475 - val_acc: 0.9887 - val_ignore_accuracy: 0.8905\n",
      "Epoch 19/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0152 - acc: 0.9973 - ignore_accuracy: 0.9721 - val_loss: 0.0470 - val_acc: 0.9888 - val_ignore_accuracy: 0.8918\n",
      "Epoch 20/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0146 - acc: 0.9974 - ignore_accuracy: 0.9732 - val_loss: 0.0464 - val_acc: 0.9888 - val_ignore_accuracy: 0.8916\n",
      "Epoch 21/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0139 - acc: 0.9975 - ignore_accuracy: 0.9741 - val_loss: 0.0459 - val_acc: 0.9889 - val_ignore_accuracy: 0.8923\n",
      "Epoch 22/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0134 - acc: 0.9976 - ignore_accuracy: 0.9753 - val_loss: 0.0455 - val_acc: 0.9890 - val_ignore_accuracy: 0.8937\n",
      "Epoch 23/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0128 - acc: 0.9977 - ignore_accuracy: 0.9765 - val_loss: 0.0451 - val_acc: 0.9890 - val_ignore_accuracy: 0.8935\n",
      "Epoch 24/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0123 - acc: 0.9977 - ignore_accuracy: 0.9768 - val_loss: 0.0447 - val_acc: 0.9891 - val_ignore_accuracy: 0.8942\n",
      "Epoch 25/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0118 - acc: 0.9978 - ignore_accuracy: 0.9778 - val_loss: 0.0444 - val_acc: 0.9891 - val_ignore_accuracy: 0.8944\n",
      "Epoch 26/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0113 - acc: 0.9979 - ignore_accuracy: 0.9787 - val_loss: 0.0440 - val_acc: 0.9891 - val_ignore_accuracy: 0.8949\n",
      "Epoch 27/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0109 - acc: 0.9980 - ignore_accuracy: 0.9795 - val_loss: 0.0437 - val_acc: 0.9891 - val_ignore_accuracy: 0.8952\n",
      "Epoch 28/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0105 - acc: 0.9981 - ignore_accuracy: 0.9803 - val_loss: 0.0435 - val_acc: 0.9892 - val_ignore_accuracy: 0.8955\n",
      "Epoch 29/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0101 - acc: 0.9981 - ignore_accuracy: 0.9810 - val_loss: 0.0432 - val_acc: 0.9892 - val_ignore_accuracy: 0.8958\n",
      "Epoch 30/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0097 - acc: 0.9982 - ignore_accuracy: 0.9821 - val_loss: 0.0429 - val_acc: 0.9893 - val_ignore_accuracy: 0.8971\n",
      "Epoch 31/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0094 - acc: 0.9983 - ignore_accuracy: 0.9830 - val_loss: 0.0427 - val_acc: 0.9893 - val_ignore_accuracy: 0.8971\n",
      "Epoch 32/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0090 - acc: 0.9984 - ignore_accuracy: 0.9837 - val_loss: 0.0425 - val_acc: 0.9894 - val_ignore_accuracy: 0.8977\n",
      "Epoch 33/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0087 - acc: 0.9985 - ignore_accuracy: 0.9849 - val_loss: 0.0423 - val_acc: 0.9894 - val_ignore_accuracy: 0.8980\n",
      "Epoch 34/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0084 - acc: 0.9986 - ignore_accuracy: 0.9855 - val_loss: 0.0421 - val_acc: 0.9895 - val_ignore_accuracy: 0.8984\n",
      "Epoch 35/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0081 - acc: 0.9986 - ignore_accuracy: 0.9863 - val_loss: 0.0419 - val_acc: 0.9895 - val_ignore_accuracy: 0.8983\n",
      "Epoch 36/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0078 - acc: 0.9987 - ignore_accuracy: 0.9868 - val_loss: 0.0418 - val_acc: 0.9895 - val_ignore_accuracy: 0.8989\n",
      "Epoch 37/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0075 - acc: 0.9987 - ignore_accuracy: 0.9873 - val_loss: 0.0416 - val_acc: 0.9896 - val_ignore_accuracy: 0.8993\n",
      "Epoch 38/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0073 - acc: 0.9988 - ignore_accuracy: 0.9878 - val_loss: 0.0415 - val_acc: 0.9896 - val_ignore_accuracy: 0.8995\n",
      "Epoch 39/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0071 - acc: 0.9989 - ignore_accuracy: 0.9883 - val_loss: 0.0413 - val_acc: 0.9896 - val_ignore_accuracy: 0.8992\n",
      "Epoch 40/40\n",
      "2504/2504 [==============================] - 10s 4ms/step - loss: 0.0068 - acc: 0.9989 - ignore_accuracy: 0.9886 - val_loss: 0.0412 - val_acc: 0.9896 - val_ignore_accuracy: 0.8996\n"
     ]
    }
   ],
   "source": [
    "history=model1.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training accuracy= 0.9970000400615575\n"
     ]
    }
   ],
   "source": [
    "print(\"Average training accuracy=\",np.mean(history.history['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783/783 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "scalars = model1.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=  99.01599026883395\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy= \",scalars[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on other samples\n",
    "test_samples = [\n",
    "    \"running is very important for me .\".split(),\n",
    "    \"I was running every day for a month .\".split()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_X = []\n",
    "for s in test_samples:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    test_samples_X.append(s_int)\n",
    "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model1.predict(test_samples_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['VBG', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
     ]
    }
   ],
   "source": [
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
