{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using nltk treebank corpus\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructuring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "sentence_tags =[]\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence=[]\n",
    "    tags=[]\n",
    "    for (s,w) in tagged_sentence:\n",
    "        sentence.append(s)\n",
    "        tags.append(w)\n",
    "    sentences.append(np.array(sentence))\n",
    "    sentence_tags.append(np.array(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Rudolph', 'Agnew', ',', '55', 'years', 'old', 'and', 'former',\n",
       "       'chairman', 'of', 'Consolidated', 'Gold', 'Fields', 'PLC', ',',\n",
       "       'was', 'named', '*-1', 'a', 'nonexecutive', 'director', 'of',\n",
       "       'this', 'British', 'industrial', 'conglomerate', '.'], dtype='<U12')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', 'CC', 'JJ', 'NN', 'IN',\n",
       "       'NNP', 'NNP', 'NNP', 'NNP', ',', 'VBD', 'VBN', '-NONE-', 'DT',\n",
       "       'JJ', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', '.'], dtype='<U6')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tags[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_sentences, test_sentences, train_tags, test_tags)=train_test_split(sentences, sentence_tags, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3131"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing word to index and tag to index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#',\n",
       " '$',\n",
       " \"''\",\n",
       " ',',\n",
       " '-LRB-',\n",
       " '-NONE-',\n",
       " '-RRB-',\n",
       " '.',\n",
       " ':',\n",
       " 'CC',\n",
       " 'CD',\n",
       " 'DT',\n",
       " 'EX',\n",
       " 'FW',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'JJR',\n",
       " 'JJS',\n",
       " 'LS',\n",
       " 'MD',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NNPS',\n",
       " 'NNS',\n",
       " 'PDT',\n",
       " 'POS',\n",
       " 'PRP',\n",
       " 'PRP$',\n",
       " 'RB',\n",
       " 'RBR',\n",
       " 'RBS',\n",
       " 'RP',\n",
       " 'SYM',\n",
       " 'TO',\n",
       " 'UH',\n",
       " 'VB',\n",
       " 'VBD',\n",
       " 'VBG',\n",
       " 'VBN',\n",
       " 'VBP',\n",
       " 'VBZ',\n",
       " 'WDT',\n",
       " 'WP',\n",
       " 'WP$',\n",
       " 'WRB',\n",
       " '``'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for out-of-vocabulary words \n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing sentences and tags to integers\n",
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    "\n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    train_sentences_X.append(s_int)\n",
    "    \n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    test_sentences_X.append(s_int)\n",
    "    \n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    "    \n",
    "for s in test_tags:\n",
    "    test_tags_y.append([tag2index[t] for t in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert sequence of tags to sequence of one hot encoded tags\n",
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#padding all sequences\n",
    "MAX_LENGTH=0\n",
    "for s in sentences:\n",
    "    MAX_LENGTH=max(MAX_LENGTH,len(s))\n",
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gauravtiwari\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model1.add(Embedding(len(word2index), 128))\n",
    "model1.add(SimpleRNN(256, return_sequences=True))\n",
    "model1.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model1.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    " \n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy',ignore_class_accuracy(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 271, 128)          1310720   \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 271, 256)          98560     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 271, 47)           12079     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 271, 47)           0         \n",
      "=================================================================\n",
      "Total params: 1,421,359\n",
      "Trainable params: 1,421,359\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gauravtiwari\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\gauravtiwari\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 2504 samples, validate on 627 samples\n",
      "Epoch 1/40\n",
      "2504/2504 [==============================] - 19s 7ms/step - loss: 2.1711 - acc: 0.7499 - ignore_accuracy: 0.0232 - val_loss: 4.1724 - val_acc: 0.4367 - val_ignore_accuracy: 0.0045\n",
      "Epoch 2/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 1.5433 - acc: 0.7807 - ignore_accuracy: 0.0500 - val_loss: 0.5653 - val_acc: 0.9000 - val_ignore_accuracy: 0.0754\n",
      "Epoch 3/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.4827 - acc: 0.9034 - ignore_accuracy: 0.0866 - val_loss: 0.4084 - val_acc: 0.9091 - val_ignore_accuracy: 0.1042\n",
      "Epoch 4/40\n",
      "2504/2504 [==============================] - 16s 6ms/step - loss: 0.3939 - acc: 0.9088 - ignore_accuracy: 0.1074 - val_loss: 0.3761 - val_acc: 0.9119 - val_ignore_accuracy: 0.1454\n",
      "Epoch 5/40\n",
      "2504/2504 [==============================] - 15s 6ms/step - loss: 0.3730 - acc: 0.9128 - ignore_accuracy: 0.1681 - val_loss: 0.3659 - val_acc: 0.9146 - val_ignore_accuracy: 0.1938\n",
      "Epoch 6/40\n",
      "2504/2504 [==============================] - 15s 6ms/step - loss: 0.3613 - acc: 0.9180 - ignore_accuracy: 0.2327 - val_loss: 0.3583 - val_acc: 0.9189 - val_ignore_accuracy: 0.2443\n",
      "Epoch 7/40\n",
      "2504/2504 [==============================] - 14s 5ms/step - loss: 0.3496 - acc: 0.9247 - ignore_accuracy: 0.2890 - val_loss: 0.3485 - val_acc: 0.9241 - val_ignore_accuracy: 0.2683\n",
      "Epoch 8/40\n",
      "2504/2504 [==============================] - 14s 5ms/step - loss: 0.3303 - acc: 0.9299 - ignore_accuracy: 0.3002 - val_loss: 0.3275 - val_acc: 0.9270 - val_ignore_accuracy: 0.2629\n",
      "Epoch 9/40\n",
      "2504/2504 [==============================] - 14s 6ms/step - loss: 0.3073 - acc: 0.9305 - ignore_accuracy: 0.2892 - val_loss: 0.3089 - val_acc: 0.9287 - val_ignore_accuracy: 0.2701\n",
      "Epoch 10/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2915 - acc: 0.9339 - ignore_accuracy: 0.3206 - val_loss: 0.2965 - val_acc: 0.9318 - val_ignore_accuracy: 0.3026\n",
      "Epoch 11/40\n",
      "2504/2504 [==============================] - 19s 7ms/step - loss: 0.2808 - acc: 0.9362 - ignore_accuracy: 0.3430 - val_loss: 0.2877 - val_acc: 0.9335 - val_ignore_accuracy: 0.3188\n",
      "Epoch 12/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.2721 - acc: 0.9381 - ignore_accuracy: 0.3619 - val_loss: 0.2801 - val_acc: 0.9360 - val_ignore_accuracy: 0.3448\n",
      "Epoch 13/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.2640 - acc: 0.9413 - ignore_accuracy: 0.3946 - val_loss: 0.2725 - val_acc: 0.9390 - val_ignore_accuracy: 0.3761\n",
      "Epoch 14/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.2554 - acc: 0.9449 - ignore_accuracy: 0.4307 - val_loss: 0.2637 - val_acc: 0.9427 - val_ignore_accuracy: 0.4150\n",
      "Epoch 15/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.2460 - acc: 0.9483 - ignore_accuracy: 0.4664 - val_loss: 0.2543 - val_acc: 0.9457 - val_ignore_accuracy: 0.4468\n",
      "Epoch 16/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.2353 - acc: 0.9508 - ignore_accuracy: 0.4926 - val_loss: 0.2432 - val_acc: 0.9486 - val_ignore_accuracy: 0.4747\n",
      "Epoch 17/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.2230 - acc: 0.9538 - ignore_accuracy: 0.5232 - val_loss: 0.2310 - val_acc: 0.9515 - val_ignore_accuracy: 0.5058\n",
      "Epoch 18/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.2093 - acc: 0.9568 - ignore_accuracy: 0.5543 - val_loss: 0.2169 - val_acc: 0.9545 - val_ignore_accuracy: 0.5338\n",
      "Epoch 19/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.1948 - acc: 0.9597 - ignore_accuracy: 0.5848 - val_loss: 0.2031 - val_acc: 0.9568 - val_ignore_accuracy: 0.5585\n",
      "Epoch 20/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.1801 - acc: 0.9621 - ignore_accuracy: 0.6092 - val_loss: 0.1895 - val_acc: 0.9589 - val_ignore_accuracy: 0.5798\n",
      "Epoch 21/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.1661 - acc: 0.9647 - ignore_accuracy: 0.6368 - val_loss: 0.1764 - val_acc: 0.9609 - val_ignore_accuracy: 0.5976\n",
      "Epoch 22/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.1533 - acc: 0.9669 - ignore_accuracy: 0.6593 - val_loss: 0.1651 - val_acc: 0.9628 - val_ignore_accuracy: 0.6175\n",
      "Epoch 23/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.1415 - acc: 0.9693 - ignore_accuracy: 0.6841 - val_loss: 0.1550 - val_acc: 0.9644 - val_ignore_accuracy: 0.6341\n",
      "Epoch 24/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.1307 - acc: 0.9717 - ignore_accuracy: 0.7089 - val_loss: 0.1458 - val_acc: 0.9667 - val_ignore_accuracy: 0.6579\n",
      "Epoch 25/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.1207 - acc: 0.9744 - ignore_accuracy: 0.7367 - val_loss: 0.1373 - val_acc: 0.9686 - val_ignore_accuracy: 0.6778\n",
      "Epoch 26/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.1114 - acc: 0.9768 - ignore_accuracy: 0.7615 - val_loss: 0.1295 - val_acc: 0.9705 - val_ignore_accuracy: 0.6979\n",
      "Epoch 27/40\n",
      "2504/2504 [==============================] - 14s 6ms/step - loss: 0.1027 - acc: 0.9791 - ignore_accuracy: 0.7854 - val_loss: 0.1221 - val_acc: 0.9723 - val_ignore_accuracy: 0.7169\n",
      "Epoch 28/40\n",
      "2504/2504 [==============================] - 16s 6ms/step - loss: 0.0945 - acc: 0.9813 - ignore_accuracy: 0.8071 - val_loss: 0.1153 - val_acc: 0.9739 - val_ignore_accuracy: 0.7332\n",
      "Epoch 29/40\n",
      "2504/2504 [==============================] - 14s 6ms/step - loss: 0.0870 - acc: 0.9832 - ignore_accuracy: 0.8276 - val_loss: 0.1088 - val_acc: 0.9754 - val_ignore_accuracy: 0.7487\n",
      "Epoch 30/40\n",
      "2504/2504 [==============================] - 13s 5ms/step - loss: 0.0800 - acc: 0.9850 - ignore_accuracy: 0.8453 - val_loss: 0.1029 - val_acc: 0.9770 - val_ignore_accuracy: 0.7650\n",
      "Epoch 31/40\n",
      "2504/2504 [==============================] - 13s 5ms/step - loss: 0.0736 - acc: 0.9865 - ignore_accuracy: 0.8611 - val_loss: 0.0976 - val_acc: 0.9782 - val_ignore_accuracy: 0.7772\n",
      "Epoch 32/40\n",
      "2504/2504 [==============================] - 13s 5ms/step - loss: 0.0677 - acc: 0.9877 - ignore_accuracy: 0.8738 - val_loss: 0.0925 - val_acc: 0.9793 - val_ignore_accuracy: 0.7888\n",
      "Epoch 33/40\n",
      "2504/2504 [==============================] - 15s 6ms/step - loss: 0.0624 - acc: 0.9888 - ignore_accuracy: 0.8854 - val_loss: 0.0879 - val_acc: 0.9803 - val_ignore_accuracy: 0.8000\n",
      "Epoch 34/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0575 - acc: 0.9898 - ignore_accuracy: 0.8956 - val_loss: 0.0838 - val_acc: 0.9813 - val_ignore_accuracy: 0.8108\n",
      "Epoch 35/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0531 - acc: 0.9908 - ignore_accuracy: 0.9053 - val_loss: 0.0799 - val_acc: 0.9822 - val_ignore_accuracy: 0.8199\n",
      "Epoch 36/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0491 - acc: 0.9917 - ignore_accuracy: 0.9144 - val_loss: 0.0768 - val_acc: 0.9829 - val_ignore_accuracy: 0.8279\n",
      "Epoch 37/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0454 - acc: 0.9924 - ignore_accuracy: 0.9221 - val_loss: 0.0734 - val_acc: 0.9837 - val_ignore_accuracy: 0.8352\n",
      "Epoch 38/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0421 - acc: 0.9931 - ignore_accuracy: 0.9289 - val_loss: 0.0707 - val_acc: 0.9843 - val_ignore_accuracy: 0.8419\n",
      "Epoch 39/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0391 - acc: 0.9936 - ignore_accuracy: 0.9344 - val_loss: 0.0682 - val_acc: 0.9849 - val_ignore_accuracy: 0.8479\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0364 - acc: 0.9940 - ignore_accuracy: 0.9388 - val_loss: 0.0659 - val_acc: 0.9853 - val_ignore_accuracy: 0.8521\n"
     ]
    }
   ],
   "source": [
    "history=model1.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training accuracy= 0.9511339418590069\n"
     ]
    }
   ],
   "source": [
    "print(\"Average training accuracy=\",np.mean(history.history['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783/783 [==============================] - 2s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "scalars = model1.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=  98.59514629247089\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy= \",scalars[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on other samples\n",
    "test_samples = [\n",
    "    \"running is very important for me .\".split(),\n",
    "    \"I was running every day for a month .\".split()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_X = []\n",
    "for s in test_samples:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    test_samples_X.append(s_int)\n",
    "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model1.predict(test_samples_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['VBG', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
     ]
    }
   ],
   "source": [
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2= Sequential()\n",
    "model2.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model2.add(Embedding(len(word2index), 128))\n",
    "model2.add(LSTM(256, return_sequences=True))\n",
    "model2.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 271, 128)          1310720   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 271, 256)          394240    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 271, 47)           12079     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 271, 47)           0         \n",
      "=================================================================\n",
      "Total params: 1,717,039\n",
      "Trainable params: 1,717,039\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy',ignore_class_accuracy(0)])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2504 samples, validate on 627 samples\n",
      "Epoch 1/40\n",
      "2504/2504 [==============================] - 71s 28ms/step - loss: 1.4786 - acc: 0.8577 - ignore_accuracy: 0.0270 - val_loss: 0.5095 - val_acc: 0.9056 - val_ignore_accuracy: 0.5637\n",
      "Epoch 2/40\n",
      "2504/2504 [==============================] - 72s 29ms/step - loss: 0.5665 - acc: 0.8876 - ignore_accuracy: 0.1139 - val_loss: 0.4005 - val_acc: 0.9063 - val_ignore_accuracy: 0.1386\n",
      "Epoch 3/40\n",
      "2504/2504 [==============================] - 64s 25ms/step - loss: 0.3981 - acc: 0.9055 - ignore_accuracy: 0.1410 - val_loss: 0.3889 - val_acc: 0.9062 - val_ignore_accuracy: 0.1631\n",
      "Epoch 4/40\n",
      " 768/2504 [========>.....................] - ETA: 49s - loss: 0.3873 - acc: 0.9042 - ignore_accuracy: 0.1422"
     ]
    }
   ],
   "source": [
    "history=model2.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average training accuracy=\",np.mean(history.history['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars = model2.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy= \",scalars[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.predict(test_samples_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
